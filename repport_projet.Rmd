---
title: "Rapport du projet"
author: "Aimé Cazeel, Khadidiatou AW, Kokou Sossou"
date: "3 février 2020"
output:
  pdf_document:
    toc: true
    number_sections: true
    df_print: kable
    keep_tex: true
    fig_caption: true
  html_document:
    df_print: paged
---

```{r, echo=FALSE, include=FALSE}
# Paramètre fixe
set.seed(10000)

#Importation packages

library(readxl)
library(ggplot2)
library(grid)
library(gridExtra)
library(mice)
library(MASS)
library(pROC)
library(plotROC)
library(caret)
library(kernlab)
library(dplyr)
library(kableExtra)

library(klaR)
library(randomForest)
library(Metrics)
library(MLmetrics)
library(missForest)

# Fonctions

cherche.ext<-function(x,taille=2){
  sigma<-sd(x,na.rm=TRUE) # pour recalculer plusieurs fois l'écart type
  ext<-mean(x,na.rm=TRUE)+c(-taille*sigma,2*sigma) # 2 valeurs de l'intervalle
  result<-(x>=ext[1])&(x<=ext[2]) # TRUE si ok, FALSE si extrême
  return(result)
  
  # on y rentre un vecteur de données
  # souvent on considère que ce qui s'écarte de 2 écart-type de la moyenne n'est pas normal
  # mais on va laisser le choix de la taille de cet intervalle (2 ou 3 sigma ?)
}

```

\newpage

# Résumé

Le CBSM ou Cognitive Behavioral Stress Management est un programme de gestion du stress qui mélange des exercices de relaxation, de restructuration cognitive et de dynamique de groupe.Si ce programme étudié principalement aux Etats-Unis s'avère efficace sur des maladies comme le cancer du sein ou le VIH, il n'existe cependant que peu d'études sur l'application du programme CBSM sur des patients atteints de maladies cardio-vasculaires et de son effet sur le stress, l'anxiété et les pensées intrusives.

Ainsi, l'objectif de ce document est d'étudier l'efficacité de ce programme auprès de patients atteints de maladies cardio-vasculaires. Pour cela, les patients ont été séparés en 2 groupes, un groupe témoin et un groupe participant au programme CBSM. Des mesures physiologiques ont étés relevés avant et après l'application du programme.

Une étude ultérieur ayant mis en valeur l'efficacité du programme concernant l'amélioration du stress perçu et de l'anxiété, nous cherchons ici à étudier l'efficacité du programme sur le stress ressenti.

Nous allons en premier lieu chercher à faire une imputation afin de conserver le maximum d'individus. En effet, à cause des codnitons de l'expérience, nous avons pu d'individus sans aucun problème.

Nous avons pu voir que plusieurs méthodes été possible pour l'imputation. La taille de nos données ne fut finalement pas spécialement contraignante et nons avons pu renvoyer des résultats satisfaisant.

Puis nous allons chercher des modèles de prédictions afin de peut-être réaliser une selection des patients rejoignant le programme. Nous pourrons voir qu'avec nos données, il semble difficile d'établir un modèle convenable. Même si nous utilisons différentes méthodes pour la recherche de predicteur efficace, nous avons au final une précision faible dans tout les cas. Il semble alors pour le moment difficile de prédire l'amélioration de l'état de santé d'un individus via de programme en se basant seulement sur des informations physilogique avant expérience.

\newpage

# Introduction

## Etat des lieux

Les maladies cardiovasculaires sont un ensemble de troubles affectant le coeur et les vaisseaux sanguins. Première cause de mortalité dans le monde selon l'OMS, elles nécessitent souvent une prise en charge lourde comprenant soutien psychologique et médicaments. Ainsi, il est important de trouver des solutions efficaces pour aider les personnes exposées à ces maladies.

Si l'alimentation et le tabagisme sont des facteurs aggravant connus, de nombreuses études mettent en évidence l'existence d'un lien entre stress et maladies cardio-vasculaires. Ainsi, proposer des solutions agissant sur le stress et ne nécessitant pas une prise en charge lourde ou médicamenteuse peut permettre de soulager les personnes atteintes de problèmes cardiovasculaires.

Or, il existe de nombreuses méthodes pour faciliter la gestion du stress, dont le programme CBSM.

## CBSM ou Cognitive Behavioral Stress Management

Le CBSM est un programme de gestion du stress qui mélange des exercices de relaxation, de restructuration cognitive et de dynamique de groupe.L'objectif est de permettre aux patients d'avoir accès à des connaisances sur eux-même, sur le stress et son impact, et sur les réactions psychologiques qu'il peut susciter. Il est constitué de plusieurs séances en groupe ainsi que d'exercices à réaliser chez soi.

Si ce programme étudié principalement aux Etats-Unis s'avére efficace sur des maladies comme le cancer du sein ou le VIH, il n'existe cependant que peu d'études sur l'application du programme CBSM sur des patients atteints de maladies cardio-vasculaires et de son effet sur le stress, l'anxiété et les pensées intrusives.

## Expérience

Ainsi en 2016 une expérience a été réalisé sur des patients atteints de pathologies cardiaques. Ces patients ont suivi le programme CBSM et ont répondus à des questionnaires afin d'évaluer leurs états psychologiques. De plus des relevés physiologiques ont aussi été réalisés. Les questionnaires doivent permettre d'évaluer le ressenti des patients par rapport au stress, tandis que les relevés physiologique permettent d'évaluer l'impact physique des interventions.

# Objectif

Ce document fait suite au travail rédigé par Franck D'ALESSANDRO mettant en avant l'efficacité du programme CBSM sur la diminution du stress perçu par les patients ainsi que leur anxiété, ainsi que le travail de Aimé CAZEEL confirmant une partie de ces résultats.

Le but de cette étude est donc d'analyser l'influence  du programme sur le stress "physique" des patients (que l'on distinguera du stress perçu).

```{r, echo=FALSE, results='hide',warning=FALSE,message=FALSE}
# Importation des données

#data_physio <- read_excel("./data/datas_physio_cbsm.xlsx",sheet=2)
data_physio <- read_excel("/Users/khadija/Desktop/Stage_HRV/CBSM cardio-data/Fichiers_CBSM_data_Excel/datas_physio_cbsm.xlsx",sheet=2)
# Formattage des noms des colonnes

# PHYSIO
##Renommage
colnames(data_physio)[5:21]<-c("T0_Mean_RR_ms","T0_STD_RR_ms","T0_Mean_HR_1_min","T0_STD_HR_1_min","T0_RMSSD_ms",
"T0_VLF_ms2","T0_LF_ms2","T0_HF_ms2","T0_VLF_pourcent","T0_LF_pourcent","T0_HF_pourcent","T0_LF_nu",
"T0_HF_nu","T0_Total_power_ms2","T0_LF_HF_ratio","T0_EDR_Hz","T0_Frequence_Respiratoire")

colnames(data_physio)[22:40]<-c("Espace_1","Espace_2","T1_Mean_RR_ms","T1_STD_RR_ms","T1_Mean_HR_1_min","T1_STD_HR_1_min","T1_RMSSD_ms",
"T1_VLF_ms2","T1_LF_ms2","T1_HF_ms2","T1_VLF_pourcent","T1_LF_pourcent","T1_HF_pourcent","T1_LF_nu",
"T1_HF_nu","T1_Total_power_ms2","T1_LF_HF_ratio","T1_EDR_Hz","T1_Frequence_Respiratoire")

colnames(data_physio)[41:57]<-c("T2_Mean_RR_ms","T2_STD_RR_ms","T2_Mean_HR_1_min","T2_STD_HR_1_min","T2_RMSSD_ms",
"T2_VLF_ms2","T2_LF_ms2","T2_HF_ms2","T2_VLF_pourcent","T2_LF_pourcent","T2_HF_pourcent","T2_LF_nu",
"T2_HF_nu","T2_Total_power_ms2","T2_LF_HF_ratio","T2_EDR_Hz","T2_Frequence_Respiratoire")

##suppression des colonnes espaces
data_physio<-data_physio[,!colnames(data_physio)%in%c("Espace_1","Espace_2")]
data_physio<-data_physio[,!colnames(data_physio)%in%c("T0_VLF_pourcent","T0_LF_pourcent","T0_HF_pourcent","T0_LF_nu","T0_HF_nu","T0_LF_HF_ratio","T0_EDR_Hz","T1_VLF_pourcent","T1_LF_pourcent","T1_HF_pourcent","T1_LF_nu","T1_HF_nu","T1_LF_HF_ratio","T1_EDR_Hz","T2_Mean_RR_ms","T2_STD_RR_ms","T2_Mean_HR_1_min","T2_STD_HR_1_min","T2_RMSSD_ms",
"T2_VLF_ms2","T2_LF_ms2","T2_HF_ms2","T2_VLF_pourcent","T2_LF_pourcent","T2_HF_pourcent","T2_LF_nu",
"T2_HF_nu","T2_Total_power_ms2","T2_LF_HF_ratio","T2_EDR_Hz","T2_Frequence_Respiratoire")]

num_var_T0<-5:14
num_var_T1<-15:24
nb_var_T0<-length(num_var_T0)
nb_var_T1<-length(num_var_T1)
```

\newpage

# Approche du problème

## Participants

Au départ, l'expérience porte sur 150 participants ayant développés une maladie cardiaque. 50 personnes participent au programme CBSM, 50 personnes participent à des séances la relaxation et 50 personnes sont des individus "contrôle" ne suivant pas de programme particulier (hormis les soins). Ces personnes proviennent de différent lieux dans l'aglomération de Grenoble :

* Service de réadaptation cardiaque de l'hôpital Sud (Echirolles)
* Institut cardio-vascualire du groupe hospitalier mutualiste de Grenoble
* Réseau des pathologies vasculaires GRANTED à Saint-Martin-d'Hères
* Service de cardiologie du CHU La Tronche
* Service de diabétologie du CHU La Tronche

Les patients sont recrutés par les équipes soignantes, les personnes acceptant de participés sont placés aléatoirement dans l'un des 3 groupes.

## Méthodes d'expérimentation

Le programme CBSM est constitué de plusieurs séances (2h par semaine) avec en plus des excercices à réaliser chez soi. Après l'ensemble des séances, les patients sont invités à répondre à des questionnaires mesurant leur perception du stress puis des mesure physiologiques sont prises. Ces mesures sont prises avec un module BIOPAC MP 150 qui va permettre de relever plusieurs variables.

Les mesures et les réponses aux questionnaires sont pris à différents moments :

* T0 : avant le début des séances CBSM
* T1 : à la fin des 10 semaines d'interventions
* T2 : 6 mois après l'intervention

### Mesure physiologique : HRV ou  Heart Rate Variability

Les recherches en psychophysiologie intègrent de plus en plus d'étude sur la variabilité du rythme cardiaque (HRV). En effet, il existe un lien entre le système nerveux parasympathique (lié à la régulation cardiaque) et de nombreux phénomènes psychophysiologique. Le HRV est d'ailleurs utilisé pour prédire les risques de mortalité provenant de cause mental ou physique.

Un relevé du HRV est simple à mettre en place et sans douleur, d'où son utilisation répandue. Parmis les nombreuses variables étudiables, celles d'intérêts sont :

* RMSSD (Root Mean Square of Succesive differences) dont les variations sont dépendantes du tonus vagal (activité du nerf vague, composant du sytème parasympathique contrôlant les activités involontaires des organes).
* HF (High Frenquencies) dont les variations proviennent aussi du tonus vagal mais peuvent être influencé par la respiration.
* LF (Low Frequencies) ainsi que le rapport LF/HF, dont les variations dépendent de divers éléments dont le système sympathique (responsable du rythme cardiaque mais aussi de la contraction des muscles lisses) et le tonus vagal.

Bien que facile à relever, le HRV est sujet à des erreurs de mesures ou à des modifications de celui-ci dû à des facteurs externes pouvant le rendre difficile à étudier (caféine etc...)

Dans les études statistiques, le HRV est très souvent utlisé comme une variable les régressions ou les corrélations, permettant souvent de distinguer des groupes selons d'autres critères (comme des différences individuelles). Parfois, le HRV peut être considéré comme une variable dépendante en créant 2 groupes séparés par la médiane. A ce moment, on suppose que le HRV illustre des particularité individuelles (on sait par exemple que le controle vagal est partiellement héritable, ce qui peut en faire une information propre à chaque individus et non dépendantes de variables externes).
 
Concernant la distribution des variables liées au HRV, la question de la normalité des variables est discutée. Mais des études tendent à observer une non normalité de la distribution de ces variables. La transformation logarithmique est alors une procédure courante pour remédier à ce problème.

## Limitations

### Erreurs

Plusieurs problèmes apparaissent dans notre méthodologie :

* Si au départ, nous devions avoir 3 groupes, au final, seulement 2 groupes existent effectivement : les groupes CBSM et CONTROLE. Ces deux groupes sont de tailles différentes. De plus, le groupe CONTROLE réalise des exercices de relaxation.
* Des erreurs de mesures peuvent fausser nos résultats (erreurs de manipulation).De plus, nous faisons face à des individus sous médication ayant une pathologie cardiaque, les chances d'obtenir des valeurs aberantes sont grandes. 

### Durées de l'expérimentation

Les individus de l'expérience ont été exposé au programme pendant 10 semaines, sans obligations d'être présent à toute les séances, ni obligation à réaliser les exercices à faire chez soi, cela limite donc l'influence du programme sur nos patients.

Enfin, cette étude est basée sur le bénévolat. Hormis la volonté des patients, il n'y avait que peu d'obligations de poursuivre l'étude. Ainsi, nous observons une très grande absence de réponse pour les temps T2 (plusieurs mois après expérience). Nous avons aussi des patients absents lors des premières mesures, mais présent après etc.

Au final, au vu du faible de nombre de réponse pour le temps T2, nous avons décider de limiter nos analyses à T0 et T1, ne nous permettant pas de constater des résultats sur le long terme.

## Objectifs de l'analyse

Ainsi, nos objectifs sont donc les suivants :

* Corriger les valeurs aberantes de certains patients
* Déterminer un modèle de prédiction de l'amélioration de l'état d'un individu.

## Méthodes et critères pour l'analyse

### Imputation

L'imputation est le processus de remplacement des données manquantes par des valeurs substituées. Plusieurs techniques d’imputations existent.

 *  la moyenne(mean): remplace l'ensemble des valeurs manquantes d'une variable par la moyenne de cette variable.
 *  Predictive mean matching(pmm): Le principe est simplement de chercher l'individus complet le plus proche de l'individus à qui il manque une valeur puis de remplacer la valeur manquante par celle de l'individus proche. C'est une méthode assez populaire et qui se présente comme assez robuste.
 *  Bayesian Linear Regression(norm): approche de la régression linéaire dans laquelle l’analyse statistique est entreprise dans le contexte de l’inférence bayésienne.
 *  random forest(rf): utilise le random forest pour imputer les valeurs manquantes
 
Afin de pouvoir évaluer une méthode, on aura tendance par la suite à minimiser certains critères comme :

 * le RMSE (Root Mean Squared Error) ou l'erreur quadratique moyenne, il s'agit simplement de l'espérance de laa racine carré des carrés de la différence entre estimation et valeure réelle.
 * le Mean absolute error qui est simplement la valeur absolue de la différence entre prediction et réalité. On peut aussi voir cette mesure selon le pourcentage d'erreur et on parlera alors de MAPE (Mean Absolute Pourcentage Error).
 
On observera aussi les statistiques descriptives avant et après les imputations pour vérifier que nous n'avons pas trop altérer nos variables.

### Modélisation

Par la suite, nous cherchons à créer un modèle pour déterminer si une personne pratiquant le programme CBSM pourra observer une amélioration physique. Cela pourra permettre de trier à l'avance les personnes. Voici une liste non exhaustive des méthodes utilisées.

#### Regression Logistique

La régression logistique est un modèle de régression binomiale. Il s'agit d'un cas particulier du modèle de régression linéaire et est souvent utilisé en apprentissage automatisé. Elle se base sur le LOGIT et est estimé via le maximum de vraissemblance.

#### SVM

Les séparateurs à vaste marge sont des techniques d'apprentissage supervisé destinées à résoudre des problèmes de discimination et de regression. Le principe est de chercher une frontière entre nos groups qui maximisent la marge, c'est à dire la distance entre la frontière et les éléments les plus proches.

#### Random Forest

Les forêts d'arbres de décisions sont des méthodes d'apprentissage basées sur les concepts de bagging (ou bootstrap) et de sous-espaces aléatoire. Le principe est d'effectuer un apprentissage d'arbre de décision sur des sous-ensembles de données.

L'arbre de décision est une méthode d'apprentissage qui cherche à découper nos données en sous-ensemble par des critères maxisimisant (selon les algorithmes) la différence entre les sous-ensembles. On sépare nos données selon une variable puis l'on réintère ce processus pour chaque sous-groupe.

Ces méthodes sont assez courantes parmis les méthodes d'apprentissage supervisé.

#### Leave-One-Out

La validation croisée est une méthode d'estimation de la fiabilité basée sur une technique d'échantillonage. Le principe est de découper nos données d'entrainements en plusieurs partition. A chaque itération, on utilisera une partition comme test et les autres comme données pour entrainer le modèle.

Le leave-one-out est un cas particulier de la validation croisée où l'on considère chaque individus comme une partition. Pour des jeux de données restreints, il permet de réaliser une validation croisée sans avoir un problème lors de l'entrainement (où l'on risquerai d'avoir très peu de données en réduisant plus encore la taille de l'échantillon d'apprentissage).

#### Bootstrap

Avec le peu de données que nous risquons d'avoir au final, nous avons besoin d'une méthode nous assurant de contrôler la "robustesse" de nos méthodes. Le bootstrap est une méthode qui se base sur le reéchantillonage avec remise. Il s'agit de considérer nos données comme la distribution de la population (pour nous, les personnes atteintes de maladie cardiaque) et donc de tirer plusieurs échantillons au sein de cette distribution.

Dans notre cas, le bootstrap est un moyen de contourner notre manque de données. Néanmoins, cette méthode dépend beaucoup de nos données de départ pour être efficace. Il faudra utiliser celle-ci avec parcimonie.

\newpage

# Premières analyses

## Données manquantes et premiers constats

Le premier problème dans notre jeu de données est la présence de données manquantes. Certaines personnes n'ont des données que concernant le temps T0 ou T1.

```{r, echo=FALSE}
# vecteur individus sans infos ) T0
vec_na_T0<-apply(X = data_physio[,num_var_T0],MARGIN = 1, FUN = function(x){sum(is.na(x))})
vec_na_T1<-apply(X = data_physio[,num_var_T1],MARGIN = 1, FUN = function(x){sum(is.na(x))})
vec_na_T0_T1<-apply(X = data_physio[,num_var_T0[1]:num_var_T1[length(num_var_T1)]],MARGIN = 1, FUN = function(x){sum(is.na(x))})
```

En effet, sur les `r nrow(data_physio)` individus du jeu de données de départ, `r sum(vec_na_T0!=0)` n'ont pas de valeurs à T0, `r sum(vec_na_T1!=0)` n'ont pas de données à T1 et `r sum(vec_na_T0_T1!=0)` n'ont pas de données à T0 et T1.

Si nous pouvons par la suite discuter de l'intérêt de conserver des individus à qui il manque des données à T0 ou T1, il semble relativement logique de ce débarasser des individus sans données à T0 et T1 (il s'agit des personnes présentes seulement au temps T2), qui seront sans intérêt dans notre cas.

```{r, echo=FALSE}
# on modifie  les données de départ en se débarassant des personnes sans données en T0 et T1
data_physio<-data_physio[vec_na_T0_T1<=nb_var_T0,]
data_physio[is.na(data_physio$`Numéro HRV`),"Numéro HRV"]<-c(9998,9999)
data_physio<-data_physio[!(data_physio$`Numéro HRV`==87&data_physio$Groupe=="CBSM"),]
data_physio[data_physio$`Numéro HRV`==24,"Numéro HRV"]<-c(24,9997)
# Recalcul de ces vecteurs
vec_na_T0<-apply(X = data_physio[,num_var_T0],MARGIN = 1, FUN = function(x){sum(is.na(x))})
vec_na_T1<-apply(X = data_physio[,num_var_T1],MARGIN = 1, FUN = function(x){sum(is.na(x))})
```

Cela nous laisse alors avec `r nrow(data_physio)`, dont `r sum(vec_na_T0==10)` sans données à T0 et `r sum(vec_na_T1==10)` sans données à T1.

## Valeurs extrêmes

Le second problème est la présence de valeurs abérantes.

```{r, echo=FALSE}
T0_indi_sp<-apply(apply(data_physio[vec_na_T0==0,num_var_T0],2, cherche.ext),1,sum)==nb_var_T0
T1_indi_sp<-apply(apply(data_physio[vec_na_T1==0,num_var_T1],2, cherche.ext),1,sum)==nb_var_T1
```

En effet, sur les `r length(T0_indi_sp)` individus du jeu de données de départ avec des valeurs à T0, `r sum(T0_indi_sp==FALSE)` ont au moins une valeur extrème, et sur les `r length(T1_indi_sp)` individus avec des valeurs à T1, `r sum(T1_indi_sp==FALSE)` ont au moins une valeur extrême.

# Imputation des données

Notre but premier est de remplacer les valeurs extrêmes chez nos individus afin de conserver un maximum d'individus pour la suite de l'étude. Pour cela, nous traitons nos données en 2 parties : nous allons premièrement imputer nos données à T0 avec l'ensemble de individus ayant des données à T0 puis nous allons imputer en T1 de la même façon.

## Imputation à T0

```{r, echo=FALSE}
# Création du jeu de données à imputer avec des NA pour les valeurs extrêmes.

imput_T0<-cbind(data_physio[vec_na_T0==0,"Numéro HRV"],apply(data_physio[vec_na_T0==0,num_var_T0],2, FUN=function(x){
  need_replace<-!(cherche.ext(x))
  xbis<-x
  xbis[need_replace]<-NA
  return(xbis)})
)
```

A T0, nous avons environ `r nrow(imput_T0)` observations et `r ncol(imput_T0)-1` variables avec `r sum(is.na(imput_T0))` valeurs manquantes. Vu la taille de nos données, nous ne pouvons pas nous permettre de supprimer ces valeurs manquantes, donc nous allons procéder à une imputation.

### Répartition des NA par variables

Regardons le taux de valeurs abérantes pour chaque varialbe à T0:
```{r,echo=FALSE}
imput_T0=imput_T0[,-1]
a=cbind(sapply(imput_T0, function(x) (sum(is.na(x)))/length(imput_T0[,1])*100))  
a=round(a,2)
colnames(a)="% NA"

a %>%
  kable() %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

```{r,echo=FALSE}
#plot the missing values
#nhanes_miss = aggr(imput_T0, col=mdc(1:2), numbers=TRUE, sortVars=TRUE, labels=names(imput_T0), cex.axis=.7, gap=3, ylab=c("Proportion of missingness","Missingness Pattern"))
```

On observe que de manière générale, les variables n'ont pas un taux trop important de valeurs manquantes sauf deux variables (RMSSD et STD).

Nous avons plusieurs choix possibles de méthode pour l'imputation. Nous allons donc devoir comparer l'efficacité de ces méthodes sur nos données.

### Comparaison des méthodes

Pour les comparer, nous allons donc volontairement retirer des valeurs parmis les données connus. Considérons notre tableau de données sans les NA, puis créons artificiellement et aléatoirement 5% de valeurs manquantes.

```{r,echo=FALSE}
#dim(imput_T0)
#sum(is.na(imput_T0))
tab=na.omit(imput_T0)
tabb <- prodNA (tab, noNA = 0.05)
```

```{r, include=FALSE}
###Méthode par la Moyenne (Mean)

#dim(tab)
imputed_Data_mean <- mice(tabb, m=1, maxit = 5, method = c('mean'), seed = 500)
```

```{r, echo=FALSE}
#Voici un summary de l'imputation:
#kable(summary(imputed_Data_mean))
```

```{r, echo=FALSE}
#<u>__Fonctions de densité: Valeurs réelles vs valeurs imputées__</u>

#densityplot(imputed_Data_mean)

#En regardant les fonctions de densité, nous remarquons que, pour la plupart des variables, les fonctions de densités des valeurs imputées(en rouge) sont assez différentes des vraies valeurs(en bleu).
#Après imputation, nous allons calculer le RMSE afin de voir l' accuracy de chaque méthode d'imputation.
```

```{r, include=FALSE}
imput_mean <- complete(imputed_Data_mean)

mae_mean<-c()
rmse_mean<-c()
mape_mean<-c()
for (i in c(1:10)){
  #mae_mean[i]<-round(mae(imput_mean[,i],tab[,i]),2)
   mae_mean[i]<-round(mae(tab[,i],imput_mean[,i]),2)

  #rmse_mean[i]<-round(rmse(imput_mean[,i],tab[,i]),2)
      rmse_mean[i]<-round(rmse(tab[,i],imput_mean[,i]),2)

  #mape_mean[i]<-round(mape(imput_mean[,i],tab[,i]),2)
    mape_mean[i]<-round(mape(tab[,i],imput_mean[,i]),2)

}
Accuracy_mean<-cbind (mae=mae_mean, rmse=rmse_mean, mape=mape_mean)
#Accuracy_mean
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, include=FALSE}
###Méthode par Predictive Mean Matching (pmm)

imputed_Data_pmm <- mice(tabb, m=1, maxit = 5, method = 'pmm', seed = 500)
#summary(imputed_Data_pmm)
#densityplot(imputed_Data_pmm)

```

```{r, include=FALSE, warning=FALSE, message=FALSE}
imput_pmm <- complete(imputed_Data_pmm)
mae_pmm<-c()
rmse_pmm<-c()
mape_pmm<-c()
for (i in c(1:10)){
    mae_pmm[i]<-round(mae(tab[,i],imput_pmm[,i]),2)
    rmse_pmm[i]<-round(rmse(tab[,i],imput_pmm[,i]),2)
    mape_pmm[i]<-round(mape(tab[,i],imput_pmm[,i]),2)

}
Accuracy_pmm<-cbind (mae=mae_pmm, rmse=rmse_pmm, mape=mape_pmm)
#Accuracy_pmm
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, include=FALSE}
###Méthode par Random Forest (rf)

imputed_Data_rf <- mice(tabb, m=1, maxit = 5, method = 'rf', seed = 500)
#summary(imputed_Data_rf)
#densityplot(imputed_Data_rf)

```

```{r, include=FALSE}
imput_rf <- complete(imputed_Data_rf)
mae_rf<-c()
rmse_rf<-c()
mape_rf<-c()
for (i in c(1:10)){
    mae_rf[i]<-round(mae(tab[,i],imput_rf[,i]),2)
    rmse_rf[i]<-round(rmse(tab[,i],imput_rf[,i]),2)
    mape_rf[i]<-round(mape(tab[,i],imput_rf[,i]),2)

}
Accuracy_rf<-cbind (mae=mae_rf, rmse=rmse_rf, mape=mape_rf)
#Accuracy_rf
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, include=FALSE}
###Méthode par Classification and Regression trees (cart)

imputed_Data_cart <- mice(tabb, m=1, maxit = 5, method = 'cart', seed = 500)
#summary(imputed_Data_cart)
#densityplot(imputed_Data_cart)

```

```{r, include=FALSE}
imput_cart <- complete(imputed_Data_cart)
mae_cart<-c()
rmse_cart<-c()
mape_cart<-c()
for (i in c(1:10)){
    mae_cart[i]<-round(mae(tab[,i],imput_cart[,i]),2)
    rmse_cart[i]<-round(rmse(tab[,i],imput_cart[,i]),2)
    mape_cart[i]<-round(mape(tab[,i],imput_cart[,i]),2)

}
Accuracy_cart<-cbind (mae=mae_cart, rmse=rmse_cart, mape=mape_cart)
#Accuracy_cart
```

```{r, include=FALSE, warning=FALSE, message=FALSE}
###Méthode par Bayesian linear regression  (norm)

imputed_Data_norm <- mice(tabb, m=1, maxit = 5, method = 'norm', seed = 500)
#summary(imputed_Data_norm)
#densityplot(imputed_Data_norm)
```

```{r, include=FALSE}
imput_norm <- complete(imputed_Data_norm)

mae_norm<-c()
rmse_norm<-c()
mape_norm<-c()
for (i in c(1:10)){
    mae_norm[i]<-round(mae(tab[,i],imput_norm[,i]),2)
    rmse_norm[i]<-round(rmse(tab[,i],imput_norm[,i]),2)
    mape_norm[i]<-round(mape(tab[,i],imput_norm[,i]),2)

}
Accuracy_norm<-cbind (mae=mae_norm, rmse=rmse_norm, mape=mape_norm)
#Accuracy_norm
```

```{r, include=FALSE}

AccurMAE=cbind.data.frame(Accuracy_mean[,1],Accuracy_pmm[,1],Accuracy_rf[,1],Accuracy_norm[,1])
AccurRMSE=cbind.data.frame(Accuracy_mean[,2],Accuracy_pmm[,2],Accuracy_rf[,2],Accuracy_norm[,2])
AccurMAPE=cbind.data.frame(Accuracy_mean[,3],Accuracy_pmm[,3],Accuracy_rf[,3],Accuracy_norm[,3])


colnames(AccurRMSE)=c("mean","pmm","rf","norm");rownames(AccurRMSE)=colnames(imput_T0);



#Scatter plot  des RMSE
Accuracy<-cbind(Methods=c("Mean","pmm","rf","norm"),
                         rbind.data.frame(Accuracy_mean,Accuracy_pmm,Accuracy_rf,Accuracy_norm))

#ggplot(data=Accuracy, aes(x=Methods, y=rmse)) +
#geom_point()

#Scatter plot  des MASE
#ggplot(data=Accuracy, aes(x=Methods, y=mape)) +
#geom_point()

#Scatter plot  des MAE
#ggplot(data=Accuracy, aes(x=Methods, y=mae)) +
#geom_point()


#A partir de ce graphique, nous voyons que ce sont les méthodes norm et pmm qui donnent les meilleures résultats.

```

```{r, echo=FALSE}
#### <u>__Comparaison des résultats__</u>
BestRmse=apply(AccurRMSE, 1, min)
R_compare<-cbind(AccurRMSE, BestRmse)


R_compare %>%
  kable() %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

#Scatter plot  des RMSE
Accuracy<-cbind(Methods=c("Mean","pmm","rf","norm"),
                         rbind.data.frame(Accuracy_mean,Accuracy_pmm,Accuracy_rf,Accuracy_norm))
```

```{r, echo=FALSE, fig.width=10}

ggplot(data=Accuracy, aes(x=Methods, y=rmse)) +
geom_point()

```

En faisant une comparaison des quatre méthodologies d’imputations, nous remarquons que les méthodes __norm__ et  __pmm__ minimisent au mieux les variables. Mais pour en choisir qu'une à la fin nous allons vérifier  les statistiques descriptives des données avant et après imputation et surtout pour la variable RMSSD en phase T1 qui servira plus tard pour la prédiction. 

### Comparaison des statistiques descriptives des méthodes et de quelques variables pour les méthodes norm  et pmm

#### Données originales:

```{r, echo=FALSE}
summary(tab[,1:5]) %>%
  kable() %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

#### Données imputées par norm

```{r, echo=FALSE}
summary(imput_norm[,1:5]) %>%
  kable() %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```

```{r, echo=FALSE, warning=FALSE, message=FALSE,fig.width=13}
densityplot(imputed_Data_norm)
```

En bleu la densité réelle, en rouge la densité après imputation.

Nous pouvons observer que la méthode norm renvoie une répartition similaire à la répartition d'origine pour la variable LF et potentiellement STD. Pour les autres variables ce n'est pas le cas, même si on observe bien des moyennes très proche.

#### Données imputées par pmm

```{r, echo=FALSE}
summary(imput_pmm[,1:5]) %>%
  kable() %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```

```{r, echo=FALSE, warning=FALSE, message=FALSE,fig.width=13}
densityplot(imputed_Data_pmm)
```

En bleu la densité réelle, en rouge la densité après imputation.

Nous pouvons observer que la méthode renvoie une bonne répartition pour les variable RMSSD et LF mais ce n'est pas autant le cas avec les autres variables.

En regardant les statistiques descriptives, nous remarquons que les valeurs de la méthode pmm se rapproche plus des vraies valeurs. D'autant plus que la méthode Norm nous a donné des valeurs négatives. Nous allons donc utiliser le pmm pour notre imputation.

\newpage

## Imputation à T1

```{r, echo=FALSE}
# Création du jeu de données à imputer avec des NA pour les valeurs extrêmes.

imput_T1<-cbind(data_physio[vec_na_T1==0,"Numéro HRV"],apply(data_physio[vec_na_T1==0,num_var_T1],2, FUN=function(x){
  need_replace<-!(cherche.ext(x))
  xbis<-x
  xbis[need_replace]<-NA
  return(xbis)})
)
head(imput_T1[,2:5]) %>%
  kable() %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

  A T1, nous avons environ `r nrow(imput_T1)` observations et `r ncol(imput_T1)-1` variables avec `r sum(is.na(imput_T1))` valeurs manquantes.

### Répartition des NA par variables

Regardons le taux de valeurs manquantes pour chaque variable à T1:

```{r, echo=FALSE}
imput=imput_T1[,-1]
b=cbind(sapply(imput, function(x) (sum(is.na(x)))/length(imput[,1])*100))  
b=round(a,2)
colnames(b)="% NA"

b %>%
  kable() %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

Vu la taille de nos données, nous pouvons dire que les taux de valeurs manquantes ne sont pas négligeables. 

### Comparaison des résultats

Considérons notre tableau de données sans les NA, puis créons artificiellement et aléatoirement 5% de valeurs manquantes. Nous obtenons ces résultats: 

```{r, include=FALSE}

T1_full<- na.omit(imput_T1[,-1])
T1P<-T1_full

#T1P valeur complete
#T1_full valeur avec données manquantes

T1_full<- prodNA(T1P, noNA = 0.05)
```

```{r, include=FALSE, warning=FALSE, message=FALSE}
## pmm

init_T1P = mice(T1_full ,maxit=5) 
predM = init_T1P$predictorMatrix
imputed_T1Ppmm <- mice(T1_full, method= c("pmm"), predictorMatrix=predM,maxit=5, m=1)
imput_T1P_pmm <- complete(imputed_T1Ppmm)

R_pmmmae<-c()
R_pmmrmse<-c()
R_pmmmape<-c()

for (i in c(1:10)){
  R_pmmmae[i]<-round(mae(imput_T1P_pmm[,i],T1P[,i]),2)
  R_pmmrmse[i]<-round(rmse(imput_T1P_pmm[,i],T1P[,i]),2)
  R_pmmmape[i]<-round(mape(imput_T1P_pmm[,i],T1P[,i]),2)
}
RAccuracy_pmm<-cbind (mae=R_pmmmae, rmse=R_pmmrmse, mape=R_pmmmape)


#

## Unconditional mean imputation (mean)

init_T1P = mice(T1_full ,maxit=5) 
predM = init_T1P$predictorMatrix
imputed_T1P <- mice(T1_full, method= c("mean"), predictorMatrix=predM,maxit=5, m=1)
imput_T1P_mean <- complete(imputed_T1P)



R_meanmae<-c()
R_meanrmse<-c()
R_meanmape<-c()

for (i in c(1:10)){
  R_meanmae[i]<-round(mae(imput_T1P_mean[,i],T1P[,i]),2)
  R_meanrmse[i]<-round(rmse(imput_T1P_mean[,i],T1P[,i]),2)
  R_meanmape[i]<-round(mape(imput_T1P_mean[,i],T1P[,i]),2)
}
RAccuracy_mean<-cbind (mae=R_pmmmae, rmse=R_pmmrmse, mape=R_pmmmape)


# Random Forest

init_T1P = mice(T1_full ,maxit=5) 
predM = init_T1P$predictorMatrix
imputed_T1P <- mice(T1_full, method= c("rf"), predictorMatrix=predM,maxit=5, m=1)
imput_T1P_rf <- complete(imputed_T1P)


R_rfmae<-c()
R_rfrmse<-c()
R_rfmape<-c()

for (i in c(1:10)){
  R_rfmae[i]<-round(mae(imput_T1P_rf[,i],T1P[,i]),2)
  R_rfrmse[i]<-round(rmse(imput_T1P_rf[,i],T1P[,i]),2)
  R_rfmape[i]<-round(mape(imput_T1P_rf[,i],T1P[,i]),2)
}
RAccuracy_rf<-cbind (mae=R_rfmae, rmse=R_rfrmse, mape=R_rfmape)


# Bayesian linear regression (norm)

init_T1P = mice(T1_full ,maxit=5) 
predM = init_T1P$predictorMatrix
imputed_T1Pnorm <- mice(T1_full, method= c("norm"), predictorMatrix=predM,maxit=5, m=1)
imput_T1P_norm <- complete(imputed_T1Pnorm)


R_normmae<-c()
R_normrmse<-c()
R_normmape<-c()

for (i in c(1:10)){
  R_normmae[i]<-round(mae(imput_T1P_norm[,i],T1P[,i]),2)
  R_normrmse[i]<-round(rmse(imput_T1P_norm[,i],T1P[,i]),2)
  R_normmape[i]<-round(mape(imput_T1P_norm[,i],T1P[,i]),2)
}
RAccuracy_norm<-cbind (mae=R_normmae, rmse=R_normrmse, mape=R_normmape)

```

```{r, echo=FALSE}
AccurMAE1=cbind.data.frame(RAccuracy_mean[,1],RAccuracy_pmm[,1],RAccuracy_rf[,1],RAccuracy_norm[,1])
AccurRMSE1=cbind.data.frame(RAccuracy_mean[,2],RAccuracy_pmm[,2],RAccuracy_rf[,2],RAccuracy_norm[,2])
AccurMAPE1=cbind.data.frame(RAccuracy_mean[,3],RAccuracy_pmm[,3],RAccuracy_rf[,3],RAccuracy_norm[,3])

colnames(AccurRMSE1)=c("mean","pmm","rf","norm");rownames(AccurRMSE1)=colnames(T1_full);
```


```{r, echo=FALSE}
R_mse<- cbind( R_pmmrmse, R_meanrmse, R_rfrmse,R_normrmse)
BestRmse=apply(AccurRMSE1, 1, min)
R_compare<-cbind(AccurRMSE1, BestRmse)


R_compare %>%
  kable() %>%
  kable_styling(bootstrap_options = "striped", full_width = F)


#Scatter plot  des RMSE
#Scatter plot  des RMSE
```

```{r, echo=FALSE,fig.width=13}
Accuracy1<-cbind(Methods=c("Mean","pmm","rf","norm"),
                         rbind.data.frame(RAccuracy_mean,RAccuracy_pmm,RAccuracy_rf,RAccuracy_norm))

ggplot(data=Accuracy1, aes(x=Methods, y=rmse)) +
geom_point()

#R_mse[, which.min(R_mse)]


#R_mse[which(min(R_mse),)]
```

Nous remarquons que les méthodes __norm__ et __pmm__ ont les RMSE les plus faibles et donnent donc de meilleurs résultats. Mais pour en choisir qu'une à la fin nous allons voir les statistiques descriptives des données avant et après imputation et surtout pour la variable RMSSD en phase T1 qui servira plus tard pour la prédiction. 


### Comparaison des statistiques descriptives de quelques variables pour les méthodes norm et pmm

#### Données originales:

```{r, echo=FALSE}
summary(T1P[,1:5]) %>%
  kable() %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

#### Données imputées par norm

```{r, echo=FALSE}
summary(imput_T1P_norm[,1:5]) %>%
  kable() %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
densityplot(imputed_T1Pnorm)
```


En bleu la densité réelle, en rouge la densité après imputation.

Nous pouvons observer que la méthode norm renvoie une répartition assez similaire à la répartition d'origine pour la variable MEAN HR et potentiellement VLF. Pour les autres variables ce n'est pas le cas, même si on observe bien des moyennes très proche.

#### Données imputées par pmm

```{r, echo=FALSE}
summary(imput_pmm[,1:5]) %>%
  kable() %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
densityplot(imputed_T1Ppmm)
```

En bleu la densité réelle, en rouge la densité après imputation.

Nous pouvons observer que la méthode renvoie une assez bonne répartition pour des variables mais ce n'est pas autant le cas pour la variable MEAN HR.

En regardant les statistiques descriptives, nous remarquons que les valeurs de la méthode pmm se rapproche plus des vraies valeurs. Globalement, les résultats sont très proche de la réalité et même si parfois, l'estimation bayesienne donne de meilleur résultat, nous pouvons observer que la méthode pmm est dans l'ensemble plus efficace.

_Conclusion_ : Notre jeu de données sera imputé par la méthode de la moyenne prévisionnelle qui donne de meilleurs résultats en T0 et T1.

```{r, echo=FALSE}
## Fichier final
imput_T0<-cbind(data_physio[vec_na_T0==0,"Numéro HRV"],apply(data_physio[vec_na_T0==0,num_var_T0],2, FUN=function(x){
  need_replace<-!(cherche.ext(x))
  xbis<-x
  xbis[need_replace]<-NA
  return(xbis)})
)

imput_T1<-cbind(data_physio[vec_na_T1==0,"Numéro HRV"],apply(data_physio[vec_na_T1==0,num_var_T1],2, FUN=function(x){
  need_replace<-!(cherche.ext(x))
  xbis<-x
  xbis[need_replace]<-NA
  return(xbis)})
)

library(dplyr)
pp=imput_T0 %>% inner_join(imput_T1, by = "Numéro HRV")
ppp=pp[,-1]
```

 Pour la suite nous allons combiner nos données en T0 et T1 afin de créer nos modèles, nous nous retrouvons donc avec `r nrow(ppp)` observations et `r ncol(ppp)` colonnes.
 

```{r, echo=FALSE, include=FALSE}
#View(pp)

imp_data <- mice(ppp, m=1, maxit = 5, method = 'pmm', seed = 500)
#densityplot(imp_data)
final <- complete(imp_data)
NumeroHRV=pp[,1]
final=cbind(NumeroHRV,final)
#View(final)

```

### Comparaison avant et après imputation

_RMSSD Avant Imputation:_

```{r, echo=FALSE}
summary(pp[,c(6,16)]) %>%
  kable() %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

_RMSSD Après Imputation:_

```{r, echo=FALSE}
summary(final[,c(6,16)]) %>%
  kable() %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

En observant par exemple pour la variable RMSSD avant imputation et après imputation, nous obtenons une moyenne de 44.79 en T0 et 39.95 en T1 contre 44.67 en T0 et 40.34 , les valeurs sont donc assez similaires.

## Bootstrap: comparaison de moyennes

Pour terminer nos analyses, nous souhaitons identifier de nouveau si il y a eu une amélioration général de l'état de santé des patients. Nous allons donc comparer les moyennes de notre échantillon. Au vu du faible de nombre de données, afin de parfaire nos estimations de la moyenne, nous avons utilisé la méthode du Bootstrap. Nous regardons surtout la variable RMSSD qui est un bon indicaeur de cette amélioration.
Nous avons donc tiré 200 échantillons, et pour chacun d'eux, avons calculé les moyennes en T0 et T1 et appliqué un Test de Wilcoxon sur ces moyennes obtenues. Nous avons donc comme hypothèses:

* H0: T0>T1
* H1: T0<T1

```{r, echo=FALSE}
T0=final[,c(1:11)]
T1=final[,c(1,12:21)]
final=cbind(NumeroHRV,final)
```

```{r, echo=FALSE,warning=FALSE}
#H0:T0>T1

x=T0$T0_RMSSD_ms
y=T1$T1_RMSSD_ms
n=length(x)
m=length(y)

wilcox=c()
M0=c()
M1=c()
B=200
wilcox=numeric(B)
M0=numeric(B)
M1=numeric(B)

for(b in 1:B){
ind1 = sample(c(1:n),n, replace = T)
ind2 = sample(c(1:m),m, replace = T)

x1=x[ind1]
y1=y[ind2]
wilcox[b] = wilcox.test(x1,y1,alternative = "greater")$p.value
M0[b]=mean(x1)
M1[b]=mean(y1)
}


#print("pval")
#wilcox
#print("M0")
#M0
#print("M1")
#M1
pval_boot=mean(wilcox)
M0_boot=mean(M0)
M1_boot=mean(M1)

#print("pval"); pval_boot; print("M0"); M0_boot; print("M1"); M1_boot;
result=cbind(M0_boot,M1_boot,pval_boot)

result %>%
  kable() %>%
  kable_styling(bootstrap_options = "striped", full_width = F)


```
Nous avons une p-value= `r pval_boot` qui est supérieure à 0.05, donc on conserve H0 au seuil de 5%; il y a bien une amélioration.

```{r,echo=FALSE}
T0=final[,c(1:11)]
T1=final[,c(1,12:21)]
final=cbind(NumeroHRV,final) #fichier qui combine les T0 et T1

# => ENTREE : imput de T0 et imput de T1

#merge_data<-merge(T0[T0$`NuméroHRV` %in% T1$`NuméroHRV`,-18],T1[,-18],by="NuméroHRV")
merge_data=final
merge_data$"progress"<-as.factor(ifelse(merge_data$T0_RMSSD_ms>merge_data$T1_RMSSD_ms,"yes","no"))

merge_data <- merge_data[,-c(1,2)]

prog=table(merge_data$progress)/nrow(merge_data)

#prog %>%
#  kable() %>%
#  kable_styling(bootstrap_options = "striped", full_width = F)

# => SORTIE : données finale avec une colonne progress pour indiquer si oui ou non il y a progré
#Nous avons 50% de patients qui ont eu une amélioration de leur condition physique.

```

# Construction de modèles

Maintenant que nous avons imputé nos variables, nous pouvons chercher à créer un modèle pour prédire si une personne a vu sa condition physique s'améliorer. Pour cela, nous n'utilisons que les variables du temps T0.

Avec moins d'une cinquantaine d'individus dans nos résultats finaux, nous décidons de simplement chercher un modèle avec l'ensemble de nos données. Il faudra alors espérer avoir plus d'individus dans le futur afin de tester nos modèles.

## Optimisation via Leave one Out

Nous allons chercher ici à optimiser nos modèles via la validation croisée. La méthode de validation croisée Leave one out permet de faire des tests avec peu d'individus.

```{r,echo=FALSE}
merge_data <- merge_data[,!(colnames(merge_data)%in%c("NumeroHRV","T1_Mean_RR_ms","T1_STD_RR_ms","T1_Mean_HR_1_min","T1_STD_HR_1_min","T1_RMSSD_ms",
"T1_VLF_ms2","T1_LF_ms2","T1_HF_ms2","T1_VLF_pourcent","T1_LF_pourcent","T1_HF_pourcent","T1_LF_nu",
"T1_HF_nu","T1_Total_power_ms2","T1_LF_HF_ratio","T1_EDR_Hz","T1_Frequence_Respiratoire"))]

train_control <- trainControl(method = "LOOCV",classProbs=T,savePredictions = T)
```

### Regression logistique

La première méthode que nous allons utiliser ici est la regression logistique descendante.

```{r,echo=FALSE,warning=FALSE}
m_glm_loo<-train(progress~.,data = merge_data, method="glmStepAIC",trControl=train_control, trace=FALSE)
```

La précision de notre modèle est d'environ `r m_glm_loo$results$Accuracy`, ce qui est légèrement meilleur qu'un modèle renvoyant toujours la même prédiction avec nos données. Les variables conservée sont les variables RMSSD, VLF, HF, Mean HR, STD RR. Les variables conservées sont principalement celles que l'on considère souvent comme assez importantes dans la litterature scientifique. Parmis ce type de variable, seul LF a été écartée.

### SVM

```{r,echo=FALSE,warning=FALSE, fig.width=10}
m_svm_loo <- train(progress ~., data = merge_data, method="svmLinear",trControl=train_control,tuneGrid=data.frame(C=c(0.05,1:50)))
#print(m_svm_loo)
plot(m_svm_loo,main="Modèle SVM")
```

Le SVM ici a dans l'ensemble un accuracy proche ou inférieur à 50%. Vu nos données, cela signifie qu'il est moins bon qu'un predicteur prédisant toujours la même valeur.

### randomForest

```{r,echo=FALSE,warning=FALSE, fig.width=10}
m_rf_loo<-train(progress ~., data = merge_data, method="rf",trControl=train_control,tuneGrid=expand.grid(.mtry=c(2,3,6:10,12:50)))
#print(m_rf)
plot(m_rf_loo,main="Modèle RandomForest")
```

Optimiser la méthode de forêt aléatoire avec la validation croisée "Leave One Out" n'est pas évidente. En effet, l'évaluation via un seul individu ne permet pas d'obtenir de résultat robuste dans ce cas là car le résultat dépend trop des variables choisies. De plus, l'accuracy, qu'importe le nombre de variable, semble se stabiliser vers 50% ou moins, ce qui est actuellement moins efficace qu'un predicteur prédisant toujours la même variable dans notre cas.

## Optimisation via Bootstrap

Ici, nous allons utiliser une autre méthode, le bootstrap pour essayer de contourner notre problème de manque de donnée.

```{r,echo=FALSE,warning=FALSE}
train_control <- trainControl(method = "boot", number=100,classProbs=T,savePredictions = T)
```

### Regression logistique

```{r,echo=FALSE,warning=FALSE}
m_glm_boot<-train(progress~.,data = merge_data, method="glmStepAIC",trControl=train_control, trace=FALSE)
```

La précision de notre modèle est d'environ `r m_glm_boot$results$Accuracy`, ce qui est légèrement meilleur qu'un modèle renvoyant toujours la même prédiction avec nos données. Les variables conservée sont les variables RMSSD, VLF, HF, Mean HR, STD RR. Les variables conservées sont principalement celles que l'on considère souvent comme assez importantes dans la litterature scientifique. Parmis ce type de variable, seul LF a été écartée.

Au final, on retrouve pratiquement le même résultat qu'avec le leave one out.


### SVM

```{r,echo=FALSE,warning=FALSE, fig.width=10}
m_svm_boot <- train(progress ~., data = merge_data, method="svmLinear",trControl=train_control,tuneGrid=data.frame(C=c(0.05,0.1,1:50)))
#print(m_svm)
plot(m_svm_boot,main="Modèle SVM")
```

Le SVM ici a dans l'ensemble un accuracy proche ou inférieur à 50%. Vu nos données, cela signifie qu'il est moins bon qu'un predicteur prédisant toujours la même valeur.

### randomForest

```{r,echo=FALSE,warning=FALSE, fig.width=10}
m_rf_boot<-train(progress ~., data = merge_data, method="rf",trControl=train_control,tuneGrid=expand.grid(.mtry=c(2,3,6:10,12:50)))
#print(m_rf)
plot(m_rf_boot,main="Modèle RandomForest")
```

L'accuracy, qu'importe le nombre de variable, semble se stabiliser vers 50% ou moins, ce qui est actuellement moins efficace qu'un predicteur prédisant toujours la même variable dans notre cas.

## Comparaisons

Nous avons vu que quelques soit les méthodes d'évaluation et de prédiction que nous utilisons, dans l'ensemble, nous obtenons souvent des résultats semblables, avec une précision proche de 50%. Nous allons maintenant terminer en comparant nos méthodes selon d'autres critères afin d'arrêter notre choix. Pour cela, nous regardons les courbes ROC.

```{r,echo=FALSE,warning=FALSE, fig.width=10}
roc_glm_loo <- ggplot(m_glm_loo$pred, aes(m=yes, d=factor(obs, levels = c("yes", "no")))) + 
  geom_roc(n.cuts=0) + 
  coord_equal() +
  style_roc()+
  ggtitle("ROC GLM avec LOO")
  
roc_glm_loo_f <-roc_glm_loo + annotate("text", x=0.75, y=0.25, label=paste("AUC =", round((calc_auc(roc_glm_loo))$AUC, 4)))

roc_glm_boot <- ggplot(m_glm_boot$pred, aes(m=yes, d=factor(obs, levels = c("yes", "no")))) + 
  geom_roc(n.cuts=0) + 
  coord_equal() +
  style_roc()+
  ggtitle("ROC GLM avec Bootstrap")

roc_glm_boot_f <-roc_glm_boot + annotate("text", x=0.75, y=0.25, label=paste("AUC =", round((calc_auc(roc_glm_boot))$AUC, 4)))

grid.arrange(roc_glm_loo_f,roc_glm_boot_f, nrow=1, ncol=2)
```

Dans l'ensemble, on peut supposer que l'AUC de la regression logistique est d'environ 0.55 .

```{r,echo=FALSE,warning=FALSE, fig.width=10}
roc_svm_loo <- ggplot(m_svm_loo$pred, aes(m=yes, d=factor(obs, levels = c("yes", "no")))) + 
  geom_roc(n.cuts=0) + 
  coord_equal() +
  style_roc()+
  ggtitle("ROC SVM avec Leave one out")

roc_svm_loo_f <-roc_svm_loo + annotate("text", x=0.75, y=0.25, label=paste("AUC =", round((calc_auc(roc_svm_loo))$AUC, 4)))

roc_svm_boot <- ggplot(m_svm_boot$pred, aes(m=yes, d=factor(obs, levels = c("yes", "no")))) + 
  geom_roc(n.cuts=0) + 
  coord_equal() +
  style_roc()+
  ggtitle("ROC SVM avec Bootstrap")

roc_svm_boot_f <-roc_svm_boot + annotate("text", x=0.75, y=0.25, label=paste("AUC =", round((calc_auc(roc_svm_boot))$AUC, 4)))

grid.arrange(roc_svm_loo_f,roc_svm_boot_f, nrow=1, ncol=2)
```

Dans l'ensemble, on peut supposer que l'AUC du SVM est d'environ 0.48 . C'est moins qu'avec la regression logistique.

```{r,echo=FALSE,warning=FALSE, fig.width=10}
roc_rf_loo <- ggplot(m_rf_loo$pred, aes(m=yes, d=factor(obs, levels = c("yes", "no")))) + 
  geom_roc(n.cuts=0) + 
  coord_equal() +
  style_roc()+
  ggtitle("ROC Random Forest avec LOO")

roc_rf_loo_f <-roc_rf_loo + annotate("text", x=0.75, y=0.25, label=paste("AUC =", round((calc_auc(roc_rf_loo))$AUC, 4)))

roc_rf_boot <- ggplot(m_rf_boot$pred, aes(m=yes, d=factor(obs, levels = c("yes", "no")))) + 
  geom_roc(n.cuts=0) + 
  coord_equal() +
  style_roc()+
  ggtitle("ROC Random Forest avec Bootstrap")

roc_rf_boot_f <-roc_rf_boot + annotate("text", x=0.75, y=0.25, label=paste("AUC =", round((calc_auc(roc_rf_boot))$AUC, 4)))

grid.arrange(roc_rf_loo_f,roc_rf_boot_f, nrow=1, ncol=2)
```

Dans l'ensemble, on peut supposer que l'AUC du random Forest est d'environ 0.43, c'est encore inférieur au SVM et à la regression logistique.

# Conclusion

Concernant l'imputation, nous avons pu voir que plusieurs méthodes etaient possibles. La taille de nos données ne fut pas spécialement contraignante et nons avons pu renvoyer des résultats satisfaisants. En sachant que les relevés physiologiques sont souvent soumis à des erreurs quand ils concernent des personnes après des problèmes cardiaques, nous pouvons aisement imaginer que l'imputation permettra de compenser cela dans des expériences futures.

Concernant les modèles de prédictions, nous pouvons voir qu'avec nos données, il semble difficile d'établir un modèle convenable. Même si nous utilisons différentes méthodes pour la recherche de predicteur efficace, nous avons au final une précision faible dans tout les cas. Il semble alors pour le moment difficile de prédire l'amélioration de l'état de santé d'un individus via le programme en se basant seulement sur des informations physilogiques avant expérience.

Enfin, pour conclure, il est à noter qu'il est possible que la méthode du RandomForest puisse renvoyer de meilleures prédictions. En effet, il suffirait d'inverser le modèle et nous aurions alors un modèle de prédiction avec une précision plus grande. Ainsi, on peut imaginer qu'avec beaucoup plus de données et surement d'itération, il est possible de voir apparaître un modèle acceptable. 